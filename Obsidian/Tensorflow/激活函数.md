##### 总结
- 首选relu函数
- 学习率设置较小值
- 输入特征标准化
- 初始参数中心化
##### 优秀的激活函数
- 非线性
- 可微性
- 单调性
- 近似恒等性

##### 激活函数输出值的范围
- 输出为有限值，基于梯度
- 输出为无限值，调小学习率

##### 常用激活函数
>Sifmoid函数

$$
f(x)=\frac{1}{1+e^{-x}}
$$
>Tanh函数
$$
f(x)=\frac{1-e^{-2x}}{1+e^{-2x}}
$$
>Relu函数
$$
f(x)=max(x,0)=
			\begin{cases}
				0, & x<0\\
				x,& x>=0
			\end{cases}
$$
